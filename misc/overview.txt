some summary points from 
"A Comprehensive Survey on Graph Neural Networks"
https://arxiv.org/pdf/1901.00596.pdf by Z. Wu, S. Pan, F. Chen, G. Long & C. Zhang

SOTA graph neural networks into 
- recurrent graph neural networks
- convolutional graph neural networks
- graph autoencoders
- spatial-temporal graph neural networks

a brief history of GNNs
* Sperduti et al. (1997): neural networks to directed acyclic graphs 
* Gori et al. (2005)~ Gallicchio et al. (2010): notion of graph neural networks outlined 
These fall into RecGNNs:
* "learn a target node's representation by propagating neighbor information in an iterative manner until 
a stable fixed point is reached.". 
* computationally expensive

ConvGNNs are divided into:
* spectral-based approaches
Bruna et al.(2013): a graph convolution based on the spectral graph theory
* spatial-based approached
Micheli et al(2009): "graph mutual dependency by architecturally composite non-recursive layers
while inheriting ideas of message passing from RecGNNs"

graph neural networks vs. network embedding
the former is designed for various tasks whereas the latter covers various kinds of methods targeting the same task.

network embedding
*represent network nodes as low-dimensional vector representations 
*preserve both network topology structure and node content information 
*subsequent task easily performed
*non-deep learning methods such as matrix factorization and random walks

graph neural networks
*explicitly extract high-level representations
*"GNNs can address the network embedding problem through a graph autoencoder framework."

graph neural networks vs. graph kernel methods 
graph kernel methods 
*dominant techniques to solve graph classification 
*employ a kernel function to measure graph similarity
*kernel-based algorithms such as SVM can be used for supervised learning 
*embed graphs or nodes into vector spaces by a deterministic(not learnable) mapping function
*computational bottlenecks

graph neural networks 
* directly perform graph classifcation based on the extracted graph representations
* much more efficient than graph kernel methods



Frameworks
*Node-level 
relate to node regression and node classification tasks

*Edge-level 
relate to edge classification and link prediction tasks 
"With two nodes' hidden representations from GNNs as inputs, 
a similarity function or a neural network can be utilized to predict the label/connection strength of an edge"

*Graph-level
relate to graph classification task
often combined with pooling and readout operations


Training Frameworks 

*Semi-supervised learning for node-level classification 
Given a single network with partial nodes labeled and others unlabeled,
can predict the class labels for the unlabeled nodes 

*Supervised learning for graph-level classification 
a combination of graph convolutional layers, graph pooling layers, and/or readout layers 
graph convolutional layers extract high-level node representations,
graph pooling layers work as down-sampling 
A readout layer "collapses node representations of each graph into a graph representation"

*Unsupervised learning for graph embedding
autoencoder framework 
negative sampling approach which samples a portion of node pairs as negative pairs (existing pairs are positive pairs)
Then a logistic regression layer is applied to distinguish between positive and negative pairs




*RecGNNs 
GNN (by Scarselli et al.) updates nodes' states by exchanging neighborhood information recurrently
until a stable equilibrium is reached.
eq (1) 
The sum makes GNN applicable to the cases where the number of neighbors differs and no neighborhood ordering is known
If f is a neural network, a penalty term has to be imposed on the Jacobian matrix
GNN alternates node state propagation and parameter gradient computation
This way, GNN can handle cyclic graphs

Graph ESN 
an encoder and an output layer. The encoder doesn't require training 
a contractive state transition fct to recurrently update node states 
afterwards, the output layer is trained by taking the fixed node states as inputs

GGNN 
uses GRU as a recurrent function
uses back-propagation through time(BPTT) algorithm to learn the parameters
runs recurrent function multiple times over all nodes. requires the intermediate states of all nodes to be stored in memory
-> not scalable

Stochastic Steady-state Embedding(SSE) 
"updates node hidden states recurrently in a stochastic and asynchronous fashion"
reccurent function is a weighted avg of the historical states and new states.





*ConvGNNs 
A. Spectral-based ConvGNNs
An undirected graph is represented by the normalized graph Laplacian matrix
since real symmetric positive semidefinite, it can be factored as L = UAUt
The graph Fourier transform to a signal x: F(x) = Utx
Due to the eigen-decomposition of the Laplacian matrix, 
(1) "any perturbation to a graph results in a change of eigenbasis"
(2) "the learned filters are domain dependent"
(3) eigen-decomposition is computationally expensive: O(n^3)

Chebyshev Spectral CNN approxiates the filter by Chebyshev polynomials of the diagonal matrix
filters defined by ChebNet are localized in space
CayleyNet applies Cayley polynomials to capture narrow frequency bands
GCN uses a first-order approx. of ChebNet 
"Adaptive GCN learns hidden structural relations unspecified by the graph adjacency matrix"

B.Spatial-based ConvGNNs 


NN4G 
learns graph mutual dependency thru a compositional neural architecture
sums a node's neighbordhood info directly
use residual connections and skip connections 
one difference from GCN: NN4G uses the unnormalized adj. matrix which may cause hidden node states to have different scales

Diffusion Convolution Neural Network (DCNN) 
graph convolutions as a diffusion process 
info. is transferred from one node to one of its neighboring nodes with a certain transition probability 
info. distribution reaches equilibrium after several rounds

Diffusion Graph Convolution (DGC)

Partition Graph Convolution (PGC)
partitions a node's neighbors into Q groups based on certain criteria
constructs Q adj. matrices then applies GCN with a different param. matrix to each neighbor group and sums the results 

Message Passing Neural Network (MPNN)
treats graph convolutions as a message passing process

Graph Isomorphism Network (GIN)
GIN is capable of distinguishing different graph structures based on the graph embedding.

GraphSage 
samples to obtain a fixed # of neighbors for each node. 

Graph Attention Network (GAT)
adopts attention to learn the relative weights btw two connected nodes.
performs multi-head attention

Mixture Model Network (MoNet)
use ode pseudo-coordinates to determine the relative position between a node and its neighbor
then map the relative position to the relative weight between the two nodes

Comparison btw spectral and spatial models

Spatial models are preferred over spectral models due to more efficiency, generality, and flexibility.
Spectral models vs Spatial models
compute eigenvectors or handle the whole graph  vs convolutions via info. propagation in a batch of nodes.
generalize poorly to new graphs vs convolutions locally on each node with shared weights 
only on undirected graphs vs flexible graph inputs(easily aggregated by aggregation function): multi-source graph inputs (e.g edge inputs), directed graphs, signed graphs, and heterogenous graphs 

C. Graph Pooling Modules

(1) reduce the size of parameters by down-sampling 
generate smaller representations and avoid overfitting, permutation invariance, 
and computational complexity
(2) readout operation to generate graph-level representation base don node representations.


graph coarsening algorithms
eigen-decomposition to coarsen graphs -> high time complexity
nowdays, mean/max/sum pooling 
Henaff et al. show that a max/mean pooling at the beginning reduces the dim. 
and mitigate the cost of the expensvie graph Gourier transform operation 


D. Discussion of Theoretical Aspects
*shape of receptive field
when compositing multiple spatial graph conv layers, the receptive field of a node grows one step farther.

*VC dimension 
VC dim of a GNN is O(p^4* n^2) or O(p^2* n), p: # of model params, n: # of nodes 

*Graph Isomorphism 
Xu et al. prove that 
if a GNN maps G1 and G2 to different embeddings,they can be regarded as non-isomorphic by the WL test of isomorphism
common GNNs such as GCN and GraphSage incapable of distinguishing different graph structures.
if aggregation and readout fcts are injective, the GNN is as powerful as WL test

*Equivariance and invariance 
A GNN must be equivariant in node-level tasks and invarianct in graph-level tasks
Components of a GNN must be invariant to node orderings.

Universal approximation
RecGNN can approximate any fct that preserves unfolding equivalence
ConvGNNs under the framework of message passing are not universal approximators of continuous functions 



*Graph autoencoders (GAEs)

A.network embedding
early approaches: mlp to build GAEs  
Deep Nueral Network for Graph Representations(DNGR): a stacked denoising autoencoder to encode/decode PPMI matrix via mlp 
Structural Deep Network Embedding(SDNE): a stacked autoencdoer to preserve the node first-order proximity and second-order proximity jointly
DNGR and SDNE  only consider node structural info such as connectivity

For graphs, # of positive node pairs << # of negative node pairs.
Convert a graph into sequences by random permutations or random walks -> deep learning approaches applicable to sequences can be used 
DRNE implicity learns network embeddings via  LSTM. to aggregate a node's neighbors.
avoids the problem that LSTM is variant to the permutation of node sequences.

NetRa has LSTMs as the encoder and the decoder.
but it ignores the node permutation variant problem of LSTM 


B. Graph Generation 
skip


*Spatial-temporal GNNs (STGNNs)

aims to model the dynamic node inputs while assuming interdependency btw connected nodes.
"Most RNN-based approaches capture spatial-temporal dep. by filtering inputs and hidden states passed 
to a recurrent unit using graph conv.

GCRN: LSTM with ChebNet. 
DCRNN: diffusion graph convolutional layer into a GRU network

Structural-RNN: a node-RNN and an edge-RNN
edge-RNNs outputs as inputs to node-RNN 
high model complexity because different RNNs for different nodes and edges 

RNN-based issues: time-consuming iterative propagation and gradient explosion/vanishing.

CNN-based approaches:
1D-CNN layers to learn temporal dependencies and graph conv. layers and spatial dependencies

CGCN: 1D-CNN layers with ChebNet or GCN layers 

Learning latent static spatial dependencies
interpretable and stable correlations among different entities in a network
"Graph WaveNet proposes a self-adaptive adjacency matrix to perform graph convolutions"


latent dynamic spatial dependencies 
GaAN: employs an attention fct to update the edge weight btw two connected nodes given their crruent inputs.
ASTGCN: spatial attention fct and a temporal attention fct 
but learning latent spatial dependencies results in high time complexity: O(n^2)


*Applications

4 main datasets: citation networks, biochemical graphs, social networks, and others
evaluation:
node classification:
mostly average accuracy or F1 score. 
but, same train/valid/test split throughout all experiments underestimates generalization error.
hyper-parameter tuning, parameter initialization, learning rate decay, and early stopping not unified across models

graph classification:
often 10-fold cross validation
but, experimental settings are ambiguous and not unified
alternative: use an external k fold cv for model assessment and an inner k fold cv for model selection

Practical Applications
computer vision: scene graph generation, point clouds classification, and action recognition
recognizing semantic relationships 

nlp: text classification
utilizes inter-relations of documents or words to label documents
nl data may contain an interal graph structure such as a syntatic dependency tree

traffic prediction 
consider traffic network as a spatial-temporal graph where
nodes : sensors on roads,
edges : distance between pairs of nodes
each node has average traffic speed within a window

Recommender systems
nodes: items and users
items and items, users and users, users and items + content info 
recommentation as a link prediction to predict missing links between users and items 
https://arxiv.org/pdf/1806.01973.pdf

Future Directions
model depth: Li et al. show that performance of a ConvGNN drops dramatically as the # of graph conv. layers increases
scalability, heterogenity, dynamicity
