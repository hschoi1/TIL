some summary points from 
"A Comprehensive Survey on Graph Neural Networks"
https://arxiv.org/pdf/1901.00596.pdf by Z. Wu, S. Pan, F. Chen, G. Long & C. Zhang

SOTA graph neural networks into 
- recurrent graph neural networks
- convolutional graph neural networks
- graph autoencoders
- spatial-temporal graph neural networks

a brief history of GNNs
* Sperduti et al. (1997): neural networks to directed acyclic graphs 
* Gori et al. (2005)~ Gallicchio et al. (2010): notion of graph neural networks outlined 
These fall into RecGNNs:
* "learn a target node's representation by propagating neighbor information in an iterative manner until 
a stable fixed point is reached.". 
* computationally expensive

ConvGNNs are divided into:
* spectral-based approaches
Bruna et al.(2013): a graph convolution based on the spectral graph theory
* spatial-based approached
Micheli et al(2009): "graph mutual dependency by architecturally composite non-recursive layers
while inheriting ideas of message passing from RecGNNs"

graph neural networks vs. network embedding
the former is designed for various tasks whereas the latter covers various kinds of methods targeting the same task.

network embedding
*represent network nodes as low-dimensional vector representations 
*preserve both network topology structure and node content information 
*subsequent task easily performed
*non-deep learning methods such as matrix factorization and random walks

graph neural networks
*explicitly extract high-level representations
*"GNNs can address the network embedding problem through a graph autoencoder framework."

graph neural networks vs. graph kernel methods 
graph kernel methods 
*dominant techniques to solve graph classification 
*employ a kernel function to measure graph similarity
*kernel-based algorithms such as SVM can be used for supervised learning 
*embed graphs or nodes into vector spaces by a deterministic(not learnable) mapping function
*computational bottlenecks

graph neural networks 
* directly perform graph classifcation based on the extracted graph representations
* much more efficient than graph kernel methods




Frameworks
*Node-level 
relate to node regression and node classification tasks

*Edge-level 
relate to edge classification and link prediction tasks 
"With two nodes' hidden representations from GNNs as inputs, 
a similarity function or a neural network can be utilized to predict the label/connection strength of an edge"

*Graph-level
relate to graph classification task
often combined with pooling and readout operations


Training Frameworks 

*Semi-supervised learning for node-level classification 
Given a single network with partial nodes labeled and others unlabeled,
can predict the class labels for the unlabeled nodes 

*Supervised learning for graph-level classification 
a combination of graph convolutional layers, graph pooling layers, and/or readout layers 
graph convolutional layers extract high-level node representations,
graph pooling layers work as down-sampling 
A readout layer "collapses node representations of each graph into a graph representation"

*Unsupervised learning for graph embedding
autoencoder framework 
negative sampling approach which samples a portion of node pairs as negative pairs (existing pairs are positive pairs)
Then a logistic regression layer is applied to distinguish between positive and negative pairs




*RecGNNs 
GNN (by Scarselli et al.) updates nodes' states by exchanging neighborhood information recurrently
until a stable equilibrium is reached.
eq (1) 
The sum makes GNN applicable to the cases where the number of neighbors differs and no neighborhood ordering is known
If f is a neural network, a penalty term has to be imposed on the Jacobian matrix
GNN alternates node state propagation and parameter gradient computation


*ConvGNNs 
A. Spectral-based ConvGNNs
An undirected graph is represented by the normalized graph Laplacian matrix
since real symmetric positive semidefinite, it can be factored as L = UAUt
The graph Fourier transform to a signal x: F(x) = Utx
Due to the eigen-decomposition of the Laplacian matrix, 
(1) "any perturbation to a graph results in a change of eigenbasis"
(2) "the learned filters are domain dependent"
(3) eigen-decomposition is computationally expensive: O(n^3)

Chebyshev Spectral CNN approxiates the filter by Chebyshev polynomials of the diagonal matrix
filters defined by ChebNet are localized in space
CayleyNet applies Cayley polynomials to capture narrow frequency bands
GCN uses a first-order approx. of ChebNet 
"Adaptive GCN learns hidden structural relations unspecified by the graph adjacency matrix"

B.Spatial-based ConvGNNs 


NN4G 
learns graph mutual dependency thru a compositional neural architecture
sums a node's neighbordhood info directly
use residual connections and skip connections 
one difference from GCN: NN4G uses the unnormalized adj. matrix which may cause hidden node states to have different scales

Diffusion Convolution Neural Network (DCNN) 
graph convolutions as a diffusion process 
info. is transferred from one node to one of its neighboring nodes with a certain transition probability 
info. distribution reaches equilibrium after several rounds

Diffusion Graph Convolution (DGC)

Partition Graph Convolution (PGC)
partitions a node's neighbors into Q groups based on certain criteria
constructs Q adj. matrices then applies GCN with a different param. matrix to each neighbor group and sums the results 

Message Passing Neural Network (MPNN)
treats graph convolutions as a message passing process

Graph Isomorphism Network (GIN)
GIN is capable of distinguishing different graph structures based on the graph embedding.

GraphSage 
samples to obtain a fixed # of neighbors for each node. 

Graph Attention Network (GAT)
adopts attention to learn the relative weights btw two connected nodes.
performs multi-head attention

Mixture Model Network (MoNet)
use ode pseudo-coordinates to determine the relative position between a node and its neighbor
then map the relative position to the relative weight between the two nodes

Comparison btw spectral and spatial models


C. Graph Pooling Modules




*Graph autoencoders (GAEs)

A.network embedding
early approaches: mlp to build GAEs  
Deep Nueral Network for Graph Representations(DNGR): a stacked denoising autoencoder to encode/decode PPMI matrix via mlp 
Structural Deep Network Embedding(SDNE): a stacked autoencdoer to preserve the node first-order proximity and second-order proximity jointly
DNGR and SDNE  only consider node structural info such as connectivity




*Spatial-temporal GNNs (STGNNs)

aims to model the dynamic node inputs while assuming interdependency btw connected nodes.
"Most RNN-based approaches capture spatial-temporal dep. by filtering inputs and hidden states passed 
to a recurrent unit using graph conv.







*Applications

4 main datasets: citation networks, biochemical graphs, social networks, and others
evaluation:
node classification:
mostly average accuracy or F1 score. 
but, same train/valid/test split throughout all experiments underestimates generalization error.
hyper-parameter tuning, parameter initialization, learning rate decay, and early stopping not unified across models

graph classification:
often 10-fold cross validation
but, experimental settings are ambiguous and not unified
alternative: use an external k fold cv for model assessment and an inner k fold cv for model selection

Practical Applications
computer vision: scene graph generation, point clouds classification, and action recognition
recognizing semantic relationships 

nlp: text classification
utilizes inter-relations of documents or words to label documents
nl data may contain an interal graph structure such as a syntatic dependency tree

traffic prediction 
consider traffic network as a spatial-temporal graph where
nodes : sensors on roads,
edges : distance between pairs of nodes
each node has average traffic speed within a window

Recommender systems
nodes: items and users
items and items, users and users, users and items + content info 
recommentation as a link prediction to predict missing links between users and items 
https://arxiv.org/pdf/1806.01973.pdf

Future Directions
model depth: Li et al. show that performance of a ConvGNN drops dramatically as the # of graph conv. layers increases
scalability, heterogenity, dynamicity
